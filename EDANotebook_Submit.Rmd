---
title: "EDA"
author: "Jaden, Sanya, and Zaim"
date: "2024-03-04"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(repos = c(CRAN = "http://cran.rstudio.com"))
```
The dataset “Eating and Drinking Places by Census Tract, United States, 2003-2017” originates from the National Neighborhood Data Archive. This dataset provides a detailed view of food services across the United States, including fast food restaurants, coffee shops, and bars. The comprehensive data supports analyses that are aimed toward the understanding of social, economic, and health-related focuses in various states and/or regions. The distribution of restaurants and the density of individuals who attend these food establishments serve as a useful resource for visualizing state-level and region-level trends. Additionally, these trends can inform policy, urban planning, and other various interventions. Our group aims to expand this analysis to observe these trends over time, focusing on food establishment preferences from 2003 and 2017 and how they may have shifted. This comparison can provide insight into dietary habits or social influences from the period that may influence decisions. Additionally, our group intends to delve into state and regional preferences which will highlight potential cultural or economic factors. Overall, this analysis will elevate understanding of American food consumption patterns, identifying key trends for further analysis and research

This dataset contains detailed information about the number and density of various types of eating and drinking establishments per census tract in the United States, measured from 2003 through 2017. The data includes counts and density measures of full-service restaurants, limited-service eating places, snack and nonalcoholic beverage bars, and drinking places that serve alcohol. Additional measures include population, land area, and employment data related to these establishments.

```{r eval=FALSE, include=FALSE}
# Install packages
install.packages("haven")
install.packages(c("ggplot2", "corrplot"))
install.packages("tidyverse")
install.packages("dplyr")
```

```{r}
library(ggplot2)
library(corrplot)
library(haven)
library(dplyr)
library(tidyverse)
library(tigris)
options(tigris_use_cache = TRUE)
```

This dataset uses TIGER census tract codes to represent the location of each observation. Each 11-digit code contains information about the state (indicated by the first two digits) and county (indicated by the following three digits) of origin. To extract this information and create new variables in our data frame for state and county categories, we will use nested loops that pull state and county IDs using the `tigris` package and run them against each observation's full TIGER code. Converting the codes to simpler categorical variables will allow us to more effectively compare data based on geographic location and increase overall readability.

First, we import the data, making sure to specify that the `tract_fips10` variable, the column representing the TIGER codes, is of the character class rather than integer. This ensures that any leading zeroes are preserved.

```{r}
# Read the csv file
nanda <- read.csv('data/nanda.csv', colClasses = c(tract_fips10 = "character"))
```

Now we can generate state and county information.

## Adding new variables for State and County

```{r}
vecfull <- nanda$tract_fips10 #saves a vector of all TIGER census tract codes
testvec <- unique(vecfull) #removes duplicates to reduce computing time
teststates <- states(year = 2010) #saves state ID information from tigris package
testnames <- teststates$STUSPS10 #saves first two digits of state TIGER codes as separate vector
testgeoids <- teststates$GEOID10 #saves abbr. state names as separate vector
statecol <- c() #this will be the new state name column for the data frame
countycol <- c() #this will be the new county name column for the data frame
for (code in testvec[1:74001]) { #last 32 IDs do not match any state/county combinations
  index = 1
  for (id in testgeoids) {
    if (substring(code, 1, 2) == id) { #check if first two digits of observation's TIGER code matches state ID
      statecol <- append(statecol, testnames[index]) #adds matching state to new state name column
      counties <- list_counties(testnames[index]) #pulls list of 3-digit county codes for the given state
      j = 1
      for (cc in counties$county_code) {
        if (substring(code, 3, 5) == cc) { #check if the next three digits of observation's TIGER code matches county ID
          countycol <- append(countycol, counties[j, 1]) #adds matching county to new county name column
          break
        } else {
          j = j + 1
        }
      }
      break
    } else {
      index = index + 1
    }
  }
}
state_county_guide <- data.frame(statecol, countycol, testvec[1:74001]) #creates data frame with state, county, and TIGER code columns
colnames(state_county_guide)[3] <- "tract_fips10" #change name of TIGER code column to prepare for joining
print(colnames(state_county_guide))
nanda_with_states <- nanda %>% left_join(state_county_guide) #add new state and county columns to data frame
```

The above code chunk runs every unique TIGER code in the dataset against the lists of state and county identifiers to create two ordered vectors of the corresponding state and county names. A new data frame is then formed from those vectors (`statecol` and `countycol`) and the vector of unique TIGER codes (`testvec`) to prepare for a left join with the rest of the data. After renaming `testvec` to `tract_fips10` to match the variable's name in `nanda`, we perform a left join to create a new data frame with the decoded state/county classifications, `nanda_with_states`.

## Additional cleaning and data overview

To begin the rest of the cleaning process, we generate an overview of the dataset's structure using `glimpse()`.

```{r}
# Get an overview of the dataset structure
glimpse(nanda_with_states)
```

First, we would like to remove rows with missing values in key fields. In particular, any rows missing measures of population or land area (`aland10`, measured in square miles) will not be of use for our exploration. Thus, we create a new data frame that filters out such observations.

```{r}
dataset_clean <- nanda_with_states %>%
  filter(!is.na(population), !is.na(aland10))
```

From here, we can filter for data from specific years to best represent our topics of interest. We are particularly interested in observing how the American food and drink industry was affected by the Great Recession and how those impacts may have differed between different types of food and drink establishments or between different geographic regions. In order to examine this potential change, we filter the data to include observations from 2007 and 2009, the years directly before and after the recession's onset in 2008. The resulting data frame is saved as `data_filtered_years`.

```{r}
data_filtered_years <- dataset_clean %>% filter(year == 2007 | year == 2009)
```

Next, we can convert `year` to a factor, allowing us to implement the years categorically rather than numerically. We achieve this by using `mutate()`.

```{r}
# Convert the 'year' column to a factor
data_filtered_years <- data_filtered_years %>% mutate(year = factor(year))

# Print the data types to confirm the change
print(sapply(data_filtered_years, class))
```

Now we can generate a summary of the newly cleaned data.

```{r}
# Inspecting the dataset for any anomalies and contextualizing through data summary
print(summary(data_filtered_years))
```

As the summary indicates, there are some wildly unusual values coming out of the data. For instance, `popden_722410`, which represents a census tract's number of bars/alcoholic drinking places per 1000 people, has a minimum value of -1 and a maximum value of 11700000, both of which are obviously impossible. This trend occurs in several of the variables representing restaurant densities per 1000 people (see `popden_sales_722513` and `popden_emp_722511`, to name a couple). Furthermore, many of these variables also contain hundreds of missing values.

```{r}
# Check for Missing Values
print(sum(is.na(data_filtered_years)))
```

To resolve these issues and clean the data further, we can eliminate any census tracts with a population of 0, since uninhabited areas will have no bearing on our study of food and drink establishments. Thus, we simply filter for observations where `population >= 1`.

```{r}
data_filtered_years <- data_filtered_years %>% filter(population >= 1)
print(summary(data_filtered_years))
```

While the updated summary still has some large outliers, the surely nonsensical values from before are gone now that `population >= 1` for all observations. The missing values in the restaurant density variables have been eliminated as well, suggesting that they were likely the result of an attempt to divide by a population of 0.

```{r}
# Check for Missing Values
print(sum(is.na(data_filtered_years)))
```

Now that the data has been sufficiently cleaned, we can start to visualize some key variables and relationships.

## Data visualization

In this section of the code, the dataset was cleaned and filter to allow further analysis through visualizations and exploration. More specifically, the data was aggregated by year which was done by grouping the dataset by 'year'. This variable was converted into a factor to provide years of interest. What was chosen was time periods that reflect pre-recession periods and post-recession periods.

In this particular part of code, the 'summarise()' function is used on this grouped data to calculate the mean of numeric variables for each year. Additionally, the 'na.rm = TRUE' code is used to ignore any missing values to ensure integrity in the calculations. The summary data displays averages of the numeric indicators such densities of certain food establishments, land area, and more.The summarization allows for deeper understanding of trends over specified years. The 'print()' function is used to show all the yearly averages.

```{r}
# Aggregate data by year, summarizing numeric values by their mean
library(dplyr)
my_data_summary <- data_filtered_years %>%
  group_by(year) %>%
  summarise(across(where(is.numeric), mean, na.rm = TRUE))

print(my_data_summary)
```

The 'data_filtered_years' dataset is grouped with the variable statecol. This variable represents the different American states. This creates a new dataset, 'my_data_bystate' to allow further analysis on a state-by-state basis.

```{r}
#group by state
my_data_bystate <- data_filtered_years %>% group_by(statecol)
```

The dataset 'my_data_state' is processed to calculate the average population for each state. This is done by using the 'summarise_at()' function to the variable 'population'. 

```{r}
#calculate mean population by state
my_data_bystate %>% summarise_at(vars(population), list(name = mean))
```

For cleaner visualizations when states are part of the axes, we filter the dataset to only include the top 5 richest and poorest states (so top 10% and bottom 10% of states by GDP per capita).

```{r}
# List of top and bottom 5 states in the US, by GDP per capita
richest_and_poorest <- c("DC", "NY", "MA", "WA", "CA", "SC", "AL", "AR", "WV", "MS")

# Filter the dataset to only include rows where the state abbreviation is in selected_states
my_data_byselect <- my_data_bystate[my_data_bystate$statecol %in%  richest_and_poorest, ]

# Arrange these 10 states in order of GDP per capita, from highest to lowest
my_data_byselect$statecol <- factor(my_data_byselect$statecol, levels = c("DC", "NY", "MA", "WA", "CA", "SC", "AL", "AR", "WV", "MS"))
```



A histogram is generated from the 'ggplot2' paclage for the variable 'count_sales_7225' which represents the number of sales. This variable is from the 'my_data_bystate' which is organized by state.

```{r}
# Histogram for a numerical column
ggplot(my_data_bystate, aes(x = count_sales_7225)) + geom_histogram()
```

This part of code shows two seperate boxplots:

The first box plot visualizes the variable 'count_sales_725' which reveals the number of sales at food establishments across many years. This boxplot is generated to display how sales vary annually according to state.

The second box plot visualizes the same variable of 'count_sales_725' but is filtered by the year 2007. The filter is used to provide insight on how sales varied by state during the year of 2007. 

```{r}
# Boxplot for a numerical column by a categorical column
ggplot(my_data_byselect, aes(x = year, y = count_sales_7225)) + geom_boxplot()
ggplot(my_data_byselect %>% filter(year == 2007), aes(x = statecol, y = count_sales_7225)) + geom_boxplot() + ggtitle("Distribution of Restaurants with Sales >0 in 2010 Census Tracts by Select States (2007)")
```

This code is generating a scatterplot to visualize the relationship between the variables 'count_emp_7225', which represents the number of employees in the establishments, and the variable 'count_sales_7225' from the 'my_data_bystate' dataset. This scatterplot is generated to explore any relationships between number of employees and sales across different states.

```{r}
# Scatterplot between two numerical columns
ggplot(my_data_bystate, aes(x = count_emp_7225, y = count_sales_7225)) + geom_point()
```

This code calculates the mean, median, and standard deviation for the 'popden_722511' variable. This variable represents the population density of full-service restaurants. This variable comes from the dataset 'data_filtered_years' . This is generated to understand the central tendency and distribution of population densities of these food establishments over the years.

```{r}
# We might be interested in the central tendency and distribution of the population density for establishments.
# For example, for full-service restaurants:
data_filtered_years %>%
  summarise(
    mean_popden_722511 = mean(popden_722511, na.rm = TRUE),
    median_popden_722511 = median(popden_722511, na.rm = TRUE),
    sd_popden_722511 = sd(popden_722511, na.rm = TRUE)
  )
```


A time series line plot is generated using 'ggplot2'. The line plot plots 'year' on the x axis and 'count_7225' on the y-axis. The 'count_7225' variable represents the count of specific food establishments. The 'geom_line(group=1, colour= "blue")' is utilized to generate a blue line as a visual representations of the data. This plot is generated to visualize trends or flucuations within the number of establishments over the years.

```{r}
# Time Series of Establishment Counts over the Years
ggplot(my_data_bystate, aes(x = year, y = count_7225)) + 
  geom_line(group=1, colour="blue") +
  labs(title = "Time Series of Establishment Counts over the Years",
       x = "Year", y = "Total Count") +
  theme_minimal()
```


This code transforms 'my_data_bystate' dataset using 'pivot_longer'. This transformation transforms the dataset to a long format which reshapes focus on columns that start with 'count_7'. These columns are turned into 'establishment_type' which represents the type of establishment. 

The visualization is a barplot which has 'establishment_type' on the x axis and 'count' on the y axis. 'geom_bar(stat='identity')' indicates that the hight of the bar is represented by the count value. This visually compares the counts of different establishments which allows one to see which types are more popular or rare. 

```{r}
library(tidyverse)

# Bar Plot of Establishment Types
my_data_long <- pivot_longer(my_data_bystate, 
                             cols = starts_with("count_7"), 
                             names_to = "establishment_type", 
                             values_to = "count")

ggplot(my_data_long, aes(x = establishment_type, y = count, fill = establishment_type)) + 
  geom_bar(stat = "identity") +
  labs(title = "Bar Plot of Establishment Types",
       x = "Establishment Type", y = "Count") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```


The 'my_data_bystate' dataset is grouped by 'statecol'. Then, the data is summarized by calculating the total count for each establishment type. This is indicated by the columns starting with 'count_7'. 'pivot_longer' transformed the data to be in a long format to make it easier to plot. 'establishment_type' is the name for more clarity on the type of establishment variable. 'mutate' modifies 'establishment_type' by removing prefix 'count_' to make it more readable. 

A bar plot is created to display each state on x-axis and their establishment counts on y-axis. The data is displayed in a descending order. The bars are color-coded by 'establishment_type' and are side-by-side. Other stylistic elements are used to make the plot more readable.

```{r}
# Summarize establishment counts by select states
establishment_counts_by_select <- my_data_byselect %>%
  group_by(statecol) %>%
  summarise(across(starts_with("count_7"), sum, na.rm = TRUE)) %>%
  pivot_longer(-statecol, names_to = "establishment_type", values_to = "count") %>%
  mutate(establishment_type = str_replace(establishment_type, "count_", ""))

# Plot
ggplot(establishment_counts_by_select, aes(x = statecol, y = count, fill = establishment_type)) + 
  geom_bar(stat = "identity", position = position_dodge(width = 0.75), width = 0.5) +  
  coord_flip() +
  labs(title = "Establishment Counts by Select States",
       x = "State", y = "Count") +
  theme_minimal() +
  theme(legend.title = element_blank())
```
```{r}
install.packages("viridis")
```

The 'viridis' package is loaded. This package is known for color scales which can help with the readability of plots. The 'my_select_data_long' dataset is grouped by 'statecol',which is the variable that represents states, and 'establishment_type'. This is summarized to calculate the mean count for each establishment type within each state.'pivot_wider' then reshapes the data to make it easier for constructing the heatmap. Within this heatmap, each row represents a state and each column represents an establishment type. Then, 'pivot_longer' is used to bring the data back to a long format which is needed for plotting with 'ggplot'. 

The heatmap is constructed by using 'ggplot'. 'establishment_type' is mapped to the x-axis and 'statecol' to the y axis. The mean count is the fill of the tiles. The 'scale_fill_viridis_c' function applies 'viridis' the fill which helps distinguish between count colors. Lastly, axis labels and plot titles were added along 'theme' which allows for the plot to be more readable. 

The purpose of this visualization is to present a comprehensive view of how the different establishment types vary state-by-state. Heatmaps are useful when trying to display magnitude a variable.
```{r}
library(viridis) 

my_select_data_long <- pivot_longer(my_data_byselect, 
                             cols = starts_with("count_7"), 
                             names_to = "establishment_type", 
                             values_to = "count")

heatmap_data <- my_select_data_long %>%
  group_by(statecol, establishment_type) %>%
  summarise(count = mean(count, na.rm = TRUE), .groups = "drop") %>%
  pivot_wider(
    names_from = establishment_type,
    values_from = count
  )

# Preparing the data for the heatmap
heatmap_data_melted <- pivot_longer(heatmap_data, cols = -statecol, names_to = "establishment_type", values_to = "count")

# Plotting
ggplot(heatmap_data_melted, aes(x = establishment_type, y = statecol, fill = count)) + 
  geom_tile() + 
  labs(title = "Heatmap of Establishment Types by Select States",
       x = "Establishment Type", y = "State") +
  scale_fill_viridis_c(option = "C") +  
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

The the total counts of establishment for each year was calculated. This is represented by year_totals, which normalizes the data across years. This is useful when comparing the data on equal terms. Then, the code merges 'year_totals' back into 'my_data_long' which creates the new dataset 'my_data_proportions'. The new dataset calculates the proportion of each establishment type per year. By doing this, raw counts are transformed into relative measures. Lastly, there is a bar plot visualization using 'ggplot'. The bars are filled by establishment type. 'scale_y_continuous' function makes the y-axis percentages which makes readability better.

This approach provides clearer understanding how establishments can contribute to the environment over the years. This accounts for annual variations in data collection and more.

```{r}
# First, calculating the total counts by year for normalization
year_totals <- my_data_long %>%
  group_by(year) %>%
  summarise(total = sum(count, na.rm = TRUE))

# Now, joining this back to the original dataset to get the proportion for each establishment type
my_data_proportions <- my_data_long %>%
  left_join(year_totals, by = "year") %>%
  mutate(proportion = count / total)

# Then, using this for plotting
ggplot(my_data_proportions, aes(x = year, y = proportion, fill = establishment_type)) +
  geom_bar(stat = "identity", position = "fill") +
  scale_y_continuous(labels = scales::percent_format()) +
  labs(title = "Distribution of Establishments by Type Over Years",
       x = "Year",
       y = "Percentage",
       fill = "Establishment Type") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

The 'my_data_long' dataset is grouped by 'statecol' and 'establishment_type'. Then, the average count of establishments for each type within each state is calculated. This helps understand the prevalence of different types of establishment across different locations geographically.

Then, the aggregated data is plotted using 'ggplot'. The states on x-axis are reordered based on '-average_count'. This makes sure that states with higher counts are shown first. 'geom_col' creates a column chart which allows one to visualize the average count. 'facet_wrap' hen divides the chart into separate panels for each establishment type. This allows for there to be side-by-side comparisons while each panel's y-axis is adjusted independently by using 'scales = "freey"'

This visualization helps identify geographic trends and disparities. Potential hotspots for establishments types are also able to be visualized. By comparing the average establishment counts across states, one can see the densities of particular establishment types.
```{r}
#Comparing the density of establishments across select states can reveal geographical trends and hotspots:
my_select_data_long %>%
  group_by(statecol, establishment_type) %>%
  summarise(average_count = mean(count, na.rm = TRUE)) %>%
  ggplot(aes(x = reorder(statecol, -average_count), y = average_count, fill = establishment_type)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~establishment_type, scales = "free_y") +
  labs(title = "Average Establishment Count by Select States and Type",
       x = "State",
       y = "Average Count") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

```{r}
install.packages("sf")
```

```{r}
install.packages("maps")
```

'states_map' variable is created for storing the data for US states using 'maps' package. 'st_as_sf' function converts the data to allow for compatibility of spatial functions in R. Then, the 'my_data_bystate' dataset is processed to calculate the total establishment count for each row. This represents the aggregated data across different types of establishments. Then, the density by state is calculated by having the dataset grouped by 'statecol'. '.groups = 'drop'' argument is used to return the standard dataframe but also not group structure.

The purpose of this code is to prepare the data to for mapping and geographic analysis.

```{r}
print(my_data_bystate)
```


```{r}
library(tidyverse)
library(sf)
library(ggplot2)
library(maps)

# Load the map data for US states
states_map <- st_as_sf(maps::map('state', plot = FALSE, fill = TRUE))


# First, calculate the total establishment count for each row
my_data_bystate <- my_data_bystate %>%
  rowwise() %>%
  mutate(total_establishment_count = sum(c_across(starts_with("count_722")), na.rm = TRUE)) %>%
  ungroup()

# Then, calculate the density by state
state_establishment_density <- my_data_bystate %>%
  group_by(statecol) %>%
  summarise(establishment_density = sum(total_establishment_count) / sum(population, na.rm = TRUE), .groups = 'drop')
```



The code establishes mapping between US states abbreviations and their corresponding full names in lowercase. 'state_abbrev_to_full' is a variable in which each US state's abbreviation is mapped to the full name in lowercase. The 'state_establishment_density' datatset undergoes a mutation in which 'statecol' is converted to the full names of the states. 'tolower' function makes sure that all the state names are lowercase uniformally. 

This code was created to provide consistency in data and allow for better readability.
```{r}
# Mapping of state abbreviations to full names in lowercase

state_abbrev_to_full <- c(
  "AL" = "alabama", "AK" = "alaska", "AZ" = "arizona", "AR" = "arkansas", "CA" = "california", 
  "CO" = "colorado", "DC" = "district of columbia", "CT" = "connecticut", "DE" = "delaware", "FL" = "florida", "GA" = "georgia", 
  "HI" = "hawaii", "ID" = "idaho", "IL" = "illinois", "IN" = "indiana", "IA" = "iowa", 
  "KS" = "kansas", "KY" = "kentucky", "LA" = "louisiana", "ME" = "maine", "MD" = "maryland", 
  "MA" = "massachusetts", "MI" = "michigan", "MN" = "minnesota", "MS" = "mississippi", "MO" = "missouri", 
  "MT" = "montana", "NE" = "nebraska", "NV" = "nevada", "NH" = "new hampshire", "NJ" = "new jersey", 
  "NM" = "new mexico", "NY" = "new york", "NC" = "north carolina", "ND" = "north dakota", "OH" = "ohio", 
  "OK" = "oklahoma", "OR" = "oregon", "PA" = "pennsylvania", "RI" = "rhode island", "SC" = "south carolina", 
  "SD" = "south dakota", "TN" = "tennessee", "TX" = "texas", "UT" = "utah", "VT" = "vermont", 
  "VA" = "virginia", "WA" = "washington", "WV" = "west virginia", "WI" = "wisconsin", "WY" = "wyoming"
)

# Convert the state abbreviations in 'statecol' to full names in lowercase

state_establishment_density <- state_establishment_density %>%
  mutate(statecol = tolower(state_abbrev_to_full[statecol]))
```

The code merges geographical data with establishment density, then it visualizes the following results on a map. 'estasblishment_state_map' combines 'states_maps' with 'state_establishment_density' by using 'left_join'. The 'by' argument aligns 'ID' column with 'statecol'. Then, the map is plotted using 'ggplot' and 'geom_sf'. The 'fill' is set to 'establishment_density' which colors each states based on establishment density.

The merging of data allows for a spatial representation  of this establishments based on different states. This can allow for further analysis on why certain states have higher densities or the opposite.

```{r}
# Join with the states map data
establishment_states_map <- states_map %>%
  left_join(state_establishment_density, by = c('ID' = 'statecol'))
```


```{r}
# Plot the map
ggplot(data = establishment_states_map) +
  geom_sf(aes(fill = establishment_density)) +
  scale_fill_continuous(low = "lightblue", high = "darkblue", na.value = "white", 
                        name = "Establishment\nDensity") +
  labs(title = "Density of Establishments by State") +
  theme_void()  # Clean theme for maps

```

This is a CSV file that contains the CDC'a 2022 data on obesity prevalence. Thos dataset includes details about percentage of obesity in each state.

```{r}
# Read CDC dataset that shows obesity prevalence by state, through % of population that has obesity

obesity_data <- read.csv('data/CDC Obesity 2022-overall-prevalence.csv')
```

This provides a view into the dataset. 

```{r}
# Quickly view the dataset

glimpse(obesity_data)

```

Shows general statistical values of obesity_data.

```{r}
# Summarize the data

summary(obesity_data)
```

The 'obesity_data' is combined with 'states_map' based on the states name. This data is merged using 'left_join" that make sures that entries from 'states_maps" are retained. This adds obesity rate to the data wherever it is available. This join is based on matching within the 'ID' column with 'state_name'.

The merged data is used to generate a geographic heat map. 'fill' is set to 'obesity_rate' which colors each state based on the prevalence of obesity. 'scale_fill_viridis_c' function is used to apply the Viridis color scale to the heat map. 

```{r}
# Merging the obesity data with the map data based on state names
obesity_states_map <- states_map %>%
  left_join(obesity_data, by = c('ID' = 'state_name'))


# Plotting the heatmap of obesity prevalence
ggplot(data = obesity_states_map) +
  geom_sf(aes(fill = obesity_rate)) +
  scale_fill_viridis_c(
    name = "Obesity Rate (%)",
    direction = -1,  # Reverse the color gradient
    option = "C"  # Viridis color option
  ) +
  labs(title = "Heatmap of Obesity Prevalence by State") +
  theme_void()  # Clean theme for maps

```

The datasets 'establishment_states_maps' and 'obesity_data' are merged. The first dataset contains data on the presence of different establishment types based on state and the second dataset provides data on obesity rate for each state. The merging is accomplished by using 'left_join' which makes sure all entries from the first dataset are supplemented by 'obesity_data'. This merging help link health metrics and other factors. This allows for greater analysis when understanding the relationship between food establishmment density and obesity rates. Overall, providing insights into potential public health issues when regarding food establishments.

```{r}
# Merge the obesity rates with the establishment density data
combined_data <- left_join(establishment_states_map, obesity_data, by = c("ID" = "state_name"))
```

The scatter plot will show the relationship between establishment density and obesity rate, thereby shedding light into public health insights, research questions, and potential interventions


```{r}
# Now let's create the scatter plot
ggplot(combined_data, aes(x = establishment_density, y = obesity_rate)) +
  geom_point() +
  geom_smooth(method = "lm", color = "blue") + # Add a linear regression line
  labs(
    title = "Correlation between Obesity Rate and Establishment Density by State",
    x = "Establishment Density",
    y = "Obesity Rate (%)"
  ) +
  theme_minimal()
```
### Conclusion

In conclusion, the visualizations created from "Eating and Drinking Places by Census Tract, United States, 2003-2017" dataset provided insight into the trends and dynamics of food establishments around America. Although this dataset is very objective and doesn't present a direct issue, it provides the tools to devise and identify one. Through the use of time series analysis, bar plots, scatter plots, density plots, and other visualizations more information was gathered to provide a direction on identifying an issue. Trends were able to be uncovered and several correlations can be used to eventually help brainstorm  a research question for the dataset. 

Overall, the visualizations has allowed us as group to further grasp the information, making it more readable and comprehensive. This provides more insight on how the data looks like and it underscores the importance of visual analysis. With the use of these visualizations, one can further analyze and understand the data as a whole. Cleaning, filtering, and visualizing the dataset has laid down a foundation for further and more advanced exploration which will come in the near future. 

### Data citation (APA)

National Neighborhood Data Archive (NaNDA). (Year). Eating and Drinking Places by Census Tract, United States, 2003-2017 [Data set]. Inter-university Consortium for Political and Social Research [distributor]. https://www.openicpsr.org/openicpsr/project/115404/version/V2/view?path=/openicpsr/115404/fcr:versions/V2.2&type=project


### References

Esposito, Michael, Li, Mao, Finlay, Jessica, Gomez-Lopez, Iris, Khan, Anam, Clarke, Philippa, and Chenoweth, Megan. National Neighborhood Data Archive (NaNDA): Eating and Drinking Places by Census Tract, United States, 2003-2017. Ann Arbor, MI: Inter-university Consortium for Political and Social Research [distributor], 2020-09-16. <https://doi.org/10.3886/E115404V2>

Adult Obesity Prevalence Maps. Centers for Disease Control and Prevention. National Center for Chronic Disease Prevention and Health Promotion, Division of Nutrition, Physical Activity, and Obesity.  (21 September 2023).

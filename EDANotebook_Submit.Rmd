---
title: "EDA"
author: "Jaden, Sanya, and Zaim"
date: "2024-03-04"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(repos = c(CRAN = "http://cran.rstudio.com"))
```
The dataset “Eating and Drinking Places by Census Tract, United States, 2003-2017” originates from the National Neighborhood Data Archive. This dataset provides a detailed view of food services across the United States, including fast food restaurants, coffee shops, and bars. The comprehensive data supports analyses that are aimed toward the understanding of social, economic, and health-related focuses in various states and/or regions. The distribution of restaurants and the density of individuals who attend these food establishments serve as a useful resource for visualizing state-level and region-level trends. Additionally, these trends can inform policy, urban planning, and other various interventions. Our group aims to expand this analysis to observe these trends, focusing on food establishments and their link to socioeconomic issues and obesity. This focus can provide insight into influences that may lead to obesity such socioeconomic status and establishment type density. These findings will highlight potential cultural or economic factors. Overall, this analysis will elevate understanding of American food consumption patterns, identifying key trends for further analysis and research. Our research question is, "How does the type of food establishments within the United States correlate with obesity rates and how do these relationships vary by socioeconomic status?"

This data set contains detailed information about the number and density of various types of eating and drinking establishments per census tract in the United States, measured from 2003 through 2017. The data includes counts and density measures of full-service restaurants, limited-service eating places, snack and nonalcoholic beverage bars, and drinking places that serve alcohol. Additional measures include population, land area, and employment data related to these establishments.The reason why this dataset was chosen in comparison to other similar datasets was due to how comprehensive it is. The data set not only provided information on establishment density, but also had information on population density, sales, number of employees. This data set allowed us to look more onto our research question as you can visualize by square area or state. Additionally, the data set covers all the establishment types which provides more insight into the regions and can be used as a good comparison for our focus on fast-food establishments.

The following are the what the variables stand for:

Count_722410 = drinking establishments 

Count_7225 = all food establishments

Count_722511 = full-service 

Count_722513 = fast food 

Count_722515 = snack establishments (e.g. coffee shops, bagel shops, ice cream parlors)

The same idea can be applied for count_sales, popden_, and other variables with the numerical codes listed above.


```{r eval=FALSE, include=FALSE}
# Install packages
install.packages("tigris")
install.packages("haven")
install.packages(c("ggplot2", "corrplot"))
install.packages("tidyverse")
install.packages("dplyr")
```

```{r}
library(ggplot2)
library(corrplot)
library(haven)
library(dplyr)
library(tidyverse)
library(tigris)
options(tigris_use_cache = TRUE)
```

This data set uses TIGER census tract codes to represent the location of each observation. Each 11-digit code contains information about the state (indicated by the first two digits) and county (indicated by the following three digits) of origin. To extract this information and create new variables in our data frame for state and county categories, we will use nested loops that pull state and county IDs using the `tigris` package and run them against each observation's full TIGER code. Converting the codes to simpler categorical variables will allow us to more effectively compare data based on geographic location and increase overall readability.

First, we import the data, making sure to specify that the `tract_fips10` variable, the column representing the TIGER codes, is of the character class rather than integer. This ensures that any leading zeros are preserved.

```{r}
# Read the csv file
nanda <- read.csv('data/nanda.csv', colClasses = c(tract_fips10 = "character"))
```

Now we can generate state and county information.

## Adding new variables for State and County

```{r}
vecfull <- nanda$tract_fips10 #saves a vector of all TIGER census tract codes
testvec <- unique(vecfull) #removes duplicates to reduce computing time
teststates <- states(year = 2010) #saves state ID information from tigris package
testnames <- teststates$STUSPS10 #saves first two digits of state TIGER codes as separate vector
testgeoids <- teststates$GEOID10 #saves abbr. state names as separate vector
statecol <- c() #this will be the new state name column for the data frame
countycol <- c() #this will be the new county name column for the data frame
for (code in testvec[1:74001]) { #last 32 IDs do not match any state/county combinations
  index = 1
  for (id in testgeoids) {
    if (substring(code, 1, 2) == id) { #check if first two digits of observation's TIGER code matches state ID
      statecol <- append(statecol, testnames[index]) #adds matching state to new state name column
      counties <- list_counties(testnames[index]) #pulls list of 3-digit county codes for the given state
      j = 1
      for (cc in counties$county_code) {
        if (substring(code, 3, 5) == cc) { #check if the next three digits of observation's TIGER code matches county ID
          countycol <- append(countycol, counties[j, 1]) #adds matching county to new county name column
          break
        } else {
          j = j + 1
        }
      }
      break
    } else {
      index = index + 1
    }
  }
}
state_county_guide <- data.frame(statecol, countycol, testvec[1:74001]) #creates data frame with state, county, and TIGER code columns
colnames(state_county_guide)[3] <- "tract_fips10" #change name of TIGER code column to prepare for joining
print(colnames(state_county_guide))
nanda_with_states <- nanda %>% left_join(state_county_guide) #add new state and county columns to data frame
```

The above code chunk runs every unique TIGER code in the dataset against the lists of state and county identifiers to create two ordered vectors of the corresponding state and county names. A new data frame is then formed from those vectors (`statecol` and `countycol`) and the vector of unique TIGER codes (`testvec`) to prepare for a left join with the rest of the data. After renaming `testvec` to `tract_fips10` to match the variable's name in `nanda`, we perform a left join to create a new data frame with the decoded state/county classifications, `nanda_with_states`.

## Additional cleaning and data overview

To begin the rest of the cleaning process, we generate an overview of the dataset's structure using `glimpse()`. The reasoning behind using glimpse is to provide a concise overview of the dataset.By doing this, we will be able to look at the various variables and decipher which one is best to investigate and continue the notebook with. Additionally, this can allow one to see if there's any missing values or areas needed for filtering and cleaning.

```{r}
# Get an overview of the dataset structure
glimpse(nanda_with_states)
```

First, we would like to remove rows with missing values in key fields. In particular, any rows missing measures of population or land area (`aland10`, measured in square miles) will not be of use for our exploration. Thus, we create a new data frame that filters out such observations.

```{r}
dataset_clean <- nanda_with_states %>%
  filter(!is.na(population), !is.na(aland10))
```

From here, we can filter for data from specific years to best represent our topics of interest. We are particularly interested in observing how the American food and drink industry was affected by the Great Recession and how those impacts may have differed between different types of food and drink establishments or between different geographic regions. In order to examine this potential change, we filter the data to include observations from 2007 and 2009, the years directly before and after the recession's onset in 2008. The resulting data frame is saved as `data_filtered_years`.

```{r}
data_filtered_years <- dataset_clean %>% filter(year == 2007 | year == 2009)
```

Next, we can convert `year` to a factor, allowing us to implement the years categorically rather than numerically. We achieve this by using `mutate()`.

```{r}
# Convert the 'year' column to a factor
data_filtered_years <- data_filtered_years %>% mutate(year = factor(year))

# Print the data types to confirm the change
print(sapply(data_filtered_years, class))
```

Now we can generate a summary of the newly cleaned data. The reason behind doing a summary of the cleaned data is to ensure that the cleaning process was effective. Additionally, this serves as one last inspection of the data set.

```{r}
# Inspecting the dataset for any anomalies and contextualizing through data summary
print(summary(data_filtered_years))
```

As the summary indicates, there are some wildly unusual values coming out of the data. For instance, `popden_722410`, which represents a census tract's number of bars/alcoholic drinking places per 1000 people, has a minimum value of -1 and a maximum value of 11700000, both of which are obviously impossible. This trend occurs in several of the variables representing restaurant densities per 1000 people (see `popden_sales_722513` and `popden_emp_722511`, to name a couple). Furthermore, many of these variables also contain hundreds of missing values.

```{r}
# Check for Missing Values
print(sum(is.na(data_filtered_years)))
```

To resolve these issues and clean the data further, we can eliminate any census tracts with a population of 0, since uninhabited areas will have no bearing on our study of food and drink establishments. Thus, we simply filter for observations where `population >= 1`.

```{r}
data_filtered_years <- data_filtered_years %>% filter(population >= 1)
print(summary(data_filtered_years))
```

While the updated summary still has some large outliers, the surely nonsensical values from before are gone now that `population >= 1` for all observations. The missing values in the restaurant density variables have been eliminated as well, suggesting that they were likely the result of an attempt to divide by a population of 0.

```{r}
# Check for Missing Values
print(sum(is.na(data_filtered_years)))
```

Now that the data has been sufficiently cleaned, we can start to visualize some key variables and relationships.

## Data visualization

In this section of the code, the dataset was cleaned and filtered to allow further analysis through visualizations and exploration. More specifically, the data was aggregated by year which was done by grouping the dataset by 'year'. This variable was converted into a factor to provide years of interest. What was chosen was time periods that reflect pre-recession periods and post-recession periods.

These data visualizations are split into two parts. One part will be exploring the variables and the dataset as a whole. Certain visualizations will include box plots regarding the economic playground and scatterplots observing variables such as sales, number of employees, and population density. 

In this particular part of the code, the 'summarise()' function is used on this grouped data to calculate the mean of numeric variables for each year. Additionally, the 'na.rm = TRUE' code is used to ignore any missing values to ensure integrity in the calculations. The summary data displays averages of the numeric indicators such as densities of certain food establishments, land area, and more.The summarization allows for deeper understanding of trends over specified years. The 'print()' function is used to show all the yearly averages.

```{r}
# Aggregate data by year, summarizing numeric values by their mean
library(dplyr)
my_data_summary <- data_filtered_years %>%
  group_by(year) %>%
  summarise(across(where(is.numeric), mean, na.rm = TRUE))

print(my_data_summary)
```

The 'data_filtered_years' data set is grouped with the variable statecol. This variable represents the different American states. This creates a new data set, 'my_data_bystate' to allow further analysis on a state-by-state basis.

```{r}
#group by state
my_data_bystate <- data_filtered_years %>% group_by(statecol)
```

The data set 'my_data_state' is processed to calculate the average population for each state. This is done by using the 'summarise_at()' function to the variable 'population'. The reasoning behind calculating the average population is to compare the number of certain establishments. A place with a greater population will most likely have a greater number of establishments. However, it is important to have more of larger ratio of population to number of food establishments or a specific food establishment.

```{r}
#calculate mean population by state
my_data_bystate %>% summarise_at(vars(population), list(name = mean))
```

For cleaner visualizations when states are part of the axes, we filter the data set to only include the top 5 richest and poorest states (so top 10% and bottom 10% of states by GDP per capita). This code is used to minimize the scope to only 10 states rather than 50. This allows for more focused visualizations and still successfully answers the research question since it creates a means for comparison of both sides.

```{r}
# List of top and bottom 5 states in the US, by GDP per capita
richest_and_poorest <- c("DC", "NY", "MA", "WA", "CA", "SC", "AL", "AR", "WV", "MS")

# Filter the dataset to only include rows where the state abbreviation is in selected_states
my_data_byselect <- my_data_bystate[my_data_bystate$statecol %in%  richest_and_poorest, ]

# Arrange these 10 states in order of GDP per capita, from highest to lowest
my_data_byselect$statecol <- factor(my_data_byselect$statecol, levels = c("DC", "NY", "MA", "WA", "CA", "SC", "AL", "AR", "WV", "MS"))
```


A histogram is generated from the 'ggplot2' package for the variable 'count_sales_7225' which represents the number of sales in all food establishments. This variable is from the 'my_data_bystate' which is organized by state. This histogram is generated to show food sales by state. Sales provides a deeper level than the number of establishments since it shows customer and state engagement with the establishments.

```{r}
# Histogram for a numerical column
ggplot(my_data_bystate, aes(x = count_sales_7225)) + geom_histogram()
```

This part of code shows two separate box plots:

The first box plot visualizes the variable 'count_sales_7225' which reveals the number of sales at food establishments across many years. This box plot is generated to display how sales vary annually according to state.This can provide insight into how time may impact certain influences such as obesity. If there is prolonged or increased sales at food establishments, this may explain an obesity issue.

The second box plot visualizes the same variable of 'count_sales_7225' according to each state but is filtered by the year 2007. The filter is used to provide insight on how food sales varied by state during the year of 2007. This can allow us to see how readily available various restaurants are in each state which can lead to a higher frequency of obesity due to access.

```{r}
# Boxplot for a numerical column by a categorical column
ggplot(my_data_byselect, aes(x = year, y = count_sales_7225)) + geom_boxplot()
ggplot(my_data_byselect %>% filter(year == 2007), aes(x = statecol, y = count_sales_7225)) + geom_boxplot() + ggtitle("Distribution of Restaurants with Sales >0 in 2007 Census Tracts by Select States (2007)")
```

This code is generating a scatter plot to visualize the relationship between the variables 'count_emp_7225', which represents the number of employees in food establishments, and the variable 'count_sales_7225' from the 'my_data_bystate' data set. This scatter plot is generated to explore any relationships between number of employees and sales across different states which can show engagement with the restaurants.

```{r}
# Scatterplot between two numerical columns
ggplot(my_data_bystate, aes(x = count_emp_7225, y = count_sales_7225)) + geom_point()
```

This code calculates the mean, median, and standard deviation for the 'popden_722511' variable. This variable represents the population density of full-service restaurants. This variable comes from the data set 'data_filtered_years' . This is generated to understand the central tendency and distribution of population densities of these food establishments over the years. The reasoning behind this code is to look at statistics of full-service restaurants as a form for comparison.

```{r}
# We might be interested in the central tendency and distribution of the population density for establishments.
# For example, for full-service restaurants:
data_filtered_years %>%
  summarise(
    mean_popden_722511 = mean(popden_722511, na.rm = TRUE),
    median_popden_722511 = median(popden_722511, na.rm = TRUE),
    sd_popden_722511 = sd(popden_722511, na.rm = TRUE)
  )
```


A time series line plot is generated using 'ggplot2'. The line plot plots 'year' on the x axis and 'count_7225' on the y-axis. The 'count_7225' variable represents the count of all food establishments. The 'geom_line(group=1, colour= "blue")' is utilized to generate a blue line as a visual representation of the data. This plot is generated to visualize trends or fluctuations within the number of establishments over the years.This code can provide insight into how the number of food establishments has changed in years to see any link to the ongoing crisis.

```{r}
# Time Series of Establishment Counts over the Years
ggplot(my_data_bystate, aes(x = year, y = count_7225)) + 
  geom_line(group=1, colour="blue") +
  labs(title = "Time Series of Establishment Counts over the Years",
       x = "Year", y = "Total Count") +
  theme_minimal()
```


This code transforms the 'my_data_bystate' dataset using 'pivot_longer'. This transformation transforms the data set to a long format which reshapes focus on columns that start with 'count_7'. These columns are turned into 'establishment_type' which represents the type of establishment. 

The visualization is a bar plot which has 'establishment_type' on the x axis and 'count' on the y axis. 'geom_bar(stat='identity')' indicates that the height of the bar is represented by the count value. This visually compares the counts of different establishments which allows one to see which types are more popular or rare. This shows the distribution of establishment types across the entire country which can reveal how the number of certain establishments may contribute to obesity or other issues.

```{r}
library(tidyverse)

# Bar Plot of Establishment Types
my_data_long <- pivot_longer(my_data_bystate, 
                             cols = starts_with("count_7"), 
                             names_to = "establishment_type", 
                             values_to = "count")

ggplot(my_data_long, aes(x = establishment_type, y = count, fill = establishment_type)) + 
  geom_bar(stat = "identity") +
  labs(title = "Bar Plot of Establishment Types",
       x = "Establishment Type", y = "Count") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```


The 'my_data_bystate' data set is grouped by 'statecol'. Then, the data is summarized by calculating the total count for each establishment type. This is indicated by the columns starting with 'count_7'. 'pivot_longer' transformed the data to be in a long format to make it easier to plot. 'establishment_type' is the name for more clarity on the type of establishment variable. 'mutate' modifies 'establishment_type' by removing the prefix 'count_' to make it more readable. 

A bar plot is created to display each state on the x-axis and their establishment counts on the y-axis. The data is displayed in a descending order. The bars are color-coded by 'establishment_type' and are side-by-side. Other stylistic elements are used to make the plot more readable. This aims to do the same thing as the previous, but focuses on each state. This can provide further insight in how socioeconomic status relates to the number of specific establishments.

```{r}
# Summarize establishment counts by select states
establishment_counts_by_select <- my_data_byselect %>%
  group_by(statecol) %>%
  summarise(across(starts_with("count_7"), sum, na.rm = TRUE)) %>%
  pivot_longer(-statecol, names_to = "establishment_type", values_to = "count") %>%
  mutate(establishment_type = str_replace(establishment_type, "count_", ""))

# Plot
ggplot(establishment_counts_by_select, aes(x = statecol, y = count, fill = establishment_type)) + 
  geom_bar(stat = "identity", position = position_dodge(width = 0.75), width = 0.5) +  
  coord_flip() +
  labs(title = "Establishment Counts by Select States",
       x = "State", y = "Count") +
  theme_minimal() +
  theme(legend.title = element_blank())
```
```{r eval=FALSE, include=FALSE}
install.packages("viridis")
```

The 'viridis' package is loaded. This package is known for color scales which can help with the readability of plots. The 'my_select_data_long' dataset is grouped by 'statecol',which is the variable that represents states, and 'establishment_type'. This is summarized to calculate the mean count for each establishment type within each state.'pivot_wider' then reshapes the data to make it easier for constructing the heatmap. Within this heatmap, each row represents a state and each column represents an establishment type. Then, 'pivot_longer' is used to bring the data back to a long format which is needed for plotting with 'ggplot'. 

The heatmap is constructed by using 'ggplot'. 'establishment_type' is mapped to the x-axis and 'statecol' to the y axis. The mean count is the fill of the tiles. The 'scale_fill_viridis_c' function applies 'viridis' to the fill which helps distinguish between count colors. Lastly, axis labels and plot titles were added along 'theme' which allows for the plot to be more readable. 

The purpose of this visualization is to present a comprehensive view of how the different establishment types vary state-by-state. Heatmaps are useful when trying to display magnitudes of a variable.
```{r}
library(viridis) 

my_select_data_long <- pivot_longer(my_data_byselect, 
                             cols = starts_with("count_7"), 
                             names_to = "establishment_type", 
                             values_to = "count")

heatmap_data <- my_select_data_long %>%
  group_by(statecol, establishment_type) %>%
  summarise(count = mean(count, na.rm = TRUE), .groups = "drop") %>%
  pivot_wider(
    names_from = establishment_type,
    values_from = count
  )

# Preparing the data for the heatmap
heatmap_data_melted <- pivot_longer(heatmap_data, cols = -statecol, names_to = "establishment_type", values_to = "count")

# Plotting
ggplot(heatmap_data_melted, aes(x = establishment_type, y = statecol, fill = count)) + 
  geom_tile() + 
  labs(title = "Heatmap of Establishment Types by Select States",
       x = "Establishment Type", y = "State") +
  scale_fill_viridis_c(option = "C") +  
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

The total counts of establishment for each year was calculated. This is represented by year_totals, which normalizes the data across years. This is useful when comparing the data on equal terms. Then, the code merges 'year_totals' back into 'my_data_long' which creates the new dataset 'my_data_proportions'. The new dataset calculates the proportion of each establishment type per year. By doing this, raw counts are transformed into relative measures. Lastly, there is a bar plot visualization using 'ggplot'. The bars are filled by establishment types. 'scale_y_continuous' function makes the y-axis percentages which makes readability better.

This approach provides clearer understanding how establishments can contribute to the environment over the years. This accounts for annual variations in data collection and more.

```{r}
# First, calculating the total counts by year for normalization
year_totals <- my_data_long %>%
  group_by(year) %>%
  summarise(total = sum(count, na.rm = TRUE))

# Now, joining this back to the original dataset to get the proportion for each establishment type
my_data_proportions <- my_data_long %>%
  left_join(year_totals, by = "year") %>%
  mutate(proportion = count / total)

# Then, using this for plotting
ggplot(my_data_proportions, aes(x = year, y = proportion, fill = establishment_type)) +
  geom_bar(stat = "identity", position = "fill") +
  scale_y_continuous(labels = scales::percent_format()) +
  labs(title = "Distribution of Establishments by Type Over Years",
       x = "Year",
       y = "Percentage",
       fill = "Establishment Type") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

The 'my_data_long' dataset is grouped by 'statecol' and 'establishment_type'. Then, the average count of establishments for each type within each state is calculated. This helps understand the prevalence of different types of establishment across different locations geographically.

Then, the aggregated data is plotted using 'ggplot'. The states on x-axis are reordered based on '-average_count'. This makes sure that states with higher counts are shown first. 'geom_col' creates a column chart which allows one to visualize the average count. 'facet_wrap' then divides the chart into separate panels for each establishment type. This allows for there to be side-by-side comparisons while each panel's y-axis is adjusted independently by using 'scales = "freey"'

This visualization helps identify geographic trends and disparities. Potential hot spots for establishment types are also able to be visualized. By comparing the average establishment counts across states, one can see the densities of particular establishment types.This provides a better visualization when wanting to compare a specific establishment type to each state rather than grouping all the establishments together in one state. One can see which state has a higher number of a specific establishment more clearly.
```{r}
#Comparing the density of establishments across select states can reveal geographical trends and hotspots:
my_select_data_long %>%
  group_by(statecol, establishment_type) %>%
  summarise(average_count = mean(count, na.rm = TRUE)) %>%
  ggplot(aes(x = reorder(statecol, -average_count), y = average_count, fill = establishment_type)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~establishment_type, scales = "free_y") +
  labs(title = "Average Establishment Count by Select States and Type",
       x = "State",
       y = "Average Count") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

```{r eval=FALSE, include=FALSE}
install.packages("sf")
```

```{r eval=FALSE, include=FALSE}
install.packages("maps")
```

'states_map' variable is created for storing the data for US states using the 'maps' package. 'st_as_sf' function converts the data to allow for compatibility of spatial functions in R. Then, the 'my_data_bystate' dataset is processed to calculate the total establishment count for each row. This represents the aggregated data across different types of establishments. Then, the density by state is calculated by having the dataset grouped by 'statecol'. '.groups = 'drop'' argument is used to return the standard data frame but also not group structure.

The purpose of this code is to prepare the data for mapping and geographic analysis.

```{r}
print(my_data_bystate)
```

This code loads the map for the United States which will allow for heatmap visualizations for each state. Then, the additional code is produced to calculate establishment density by state. In total, this code is used in order to prepare for future visualizations that involve a heat map of density of the visual of America. Each state will have a different color according to the density scale to showcase state-by-state data.
```{r}
library(tidyverse)
library(sf)
library(ggplot2)
library(maps)

# Load the map data for United states
states_map <- st_as_sf(maps::map('state', plot = FALSE, fill = TRUE))


# First, calculate the total establishment count for each row
my_data_bystate <- my_data_bystate %>%
  rowwise() %>%
  mutate(total_establishment_count = sum(c_across(starts_with("count_722")), na.rm = TRUE)) %>%
  ungroup()

# Then, calculate the density by state
state_establishment_density <- my_data_bystate %>%
  group_by(statecol) %>%
  summarise(establishment_density = sum(total_establishment_count) / sum(population, na.rm = TRUE), .groups = 'drop')
```



The code establishes mapping between US states abbreviations and their corresponding full names in lowercase. 'state_abbrev_to_full' is a variable in which each US state's abbreviation is mapped to the full name in lowercase. The 'state_establishment_density' dataset undergoes a mutation in which 'statecol' is converted to the full names of the states. 'tolower' function makes sure that all the state names are lowercase uniformly. 

This code was created to provide consistency in data and allow for better readability.
```{r}
# Mapping of state abbreviations to full names in lowercase

state_abbrev_to_full <- c(
  "AL" = "alabama", "AK" = "alaska", "AZ" = "arizona", "AR" = "arkansas", "CA" = "california", 
  "CO" = "colorado", "DC" = "district of columbia", "CT" = "connecticut", "DE" = "delaware", "FL" = "florida", "GA" = "georgia", 
  "HI" = "hawaii", "ID" = "idaho", "IL" = "illinois", "IN" = "indiana", "IA" = "iowa", 
  "KS" = "kansas", "KY" = "kentucky", "LA" = "louisiana", "ME" = "maine", "MD" = "maryland", 
  "MA" = "massachusetts", "MI" = "michigan", "MN" = "minnesota", "MS" = "mississippi", "MO" = "missouri", 
  "MT" = "montana", "NE" = "nebraska", "NV" = "nevada", "NH" = "new hampshire", "NJ" = "new jersey", 
  "NM" = "new mexico", "NY" = "new york", "NC" = "north carolina", "ND" = "north dakota", "OH" = "ohio", 
  "OK" = "oklahoma", "OR" = "oregon", "PA" = "pennsylvania", "RI" = "rhode island", "SC" = "south carolina", 
  "SD" = "south dakota", "TN" = "tennessee", "TX" = "texas", "UT" = "utah", "VT" = "vermont", 
  "VA" = "virginia", "WA" = "washington", "WV" = "west virginia", "WI" = "wisconsin", "WY" = "wyoming"
)

# Convert the state abbreviations in 'statecol' to full names in lowercase

state_establishment_density <- state_establishment_density %>%
  mutate(statecol = tolower(state_abbrev_to_full[statecol]))
```

The code merges geographical data with establishment density, then it visualizes the following results on a map. 'estasblishment_state_map' combines 'states_maps' with 'state_establishment_density' by using 'left_join'. The 'by' argument aligns the 'ID' column with 'statecol'. Then, the map is plotted using 'ggplot' and 'geom_sf'. The 'fill' is set to 'establishment_density' which colors each state based on establishment density.

The merging of data allows for a spatial representation  of these establishments based on different states. This can allow for further analysis on why certain states have higher densities or the opposite.

```{r}
# Join with the states map data
establishment_states_map <- states_map %>%
  left_join(state_establishment_density, by = c('ID' = 'statecol'))
```

This plot is showcasing the density of establishments in general by state. This can be useful when comparing population density, socioeconomic status, and obesity rates to the number of establishments which may be a plausible influence.

```{r}
# Plot the map
ggplot(data = establishment_states_map) +
  geom_sf(aes(fill = establishment_density)) +
  scale_fill_continuous(low = "lightblue", high = "darkblue", na.value = "white", 
                        name = "Establishment\nDensity") +
  labs(title = "Density of Establishments by State") +
  theme_void()  # Clean theme for maps

```

This is a CSV file that contains the CDC's 2022 data on obesity prevalence. This data set includes details about the percentage of obesity in each state.

```{r}
# Read CDC data set that shows obesity prevalence by state, through % of population that has obesity

obesity_data <- read.csv('data/CDC Obesity 2022-overall-prevalence.csv')
```

This provides a view into the data set. The glimpse allows us to look at all the variables within the data set, which can help direct further steps in using this data set.

```{r}
# Quickly view the data set

glimpse(obesity_data)

```

The summary function is used to show a general statistical values of obesity_data.Therefore, we used this to get a general idea of the distributions.

```{r}
# Summarize the data

summary(obesity_data)
```

The 'obesity_data' is combined with 'states_map' based on the states name. This data is merged using 'left_join" that makes sure that entries from 'states_maps" are retained. This adds obesity rate to the data wherever it is available. This join is based on matching within the 'ID' column with 'state_name'.

The merged data is used to generate a geographic heat map. 'fill' is set to 'obesity_rate' which colors each state based on the prevalence of obesity. 'scale_fill_viridis_c' function is used to apply the Viridis color scale to the heat map. The obesity heatmap is used to identify obesity levels according to socioeconomic status which is revealed by our chosen states.

```{r}
# Merging the obesity data with the map data based on state names
obesity_states_map <- states_map %>%
  left_join(obesity_data, by = c('ID' = 'state_name'))


# Plotting the heatmap of obesity prevalence
ggplot(data = obesity_states_map) +
  geom_sf(aes(fill = obesity_rate)) +
  scale_fill_viridis_c(
    name = "Obesity Rate (%)",
    direction = -1,  # Reverse the color gradient
    option = "C"  # Viridis color option
  ) +
  labs(title = "Heatmap of Obesity Prevalence by State") +
  theme_void()  # Clean theme for maps

```

The data sets 'establishment_states_maps' and 'obesity_data' are merged. The first data set contains data on the presence of different establishment types based on state and the second data set provides data on obesity rate for each state. The merging is accomplished by using 'left_join' which makes sure all entries from the first data set are supplemented by 'obesity_data'. This merging helps link health metrics and other factors. This allows for greater analysis when understanding the relationship between food establishment density and obesity rates. Overall, providing insights into potential public health issues when regarding food establishments.

```{r}
# Merge the obesity rates with the establishment density data
state_establishment_density <- state_establishment_density %>% rename('state_name' = 'statecol')
combined_data <- obesity_data %>% left_join(state_establishment_density)
```

The scatter plot will show the relationship between establishment density and obesity rate, thereby shedding light onto how food establishments can directly impact obesity. This helps further our research question of finding the link to obesity rates.


```{r}
# Now let's create the scatter plot
ggplot(combined_data, aes(x = establishment_density, y = obesity_rate)) +
  geom_point() +
  geom_smooth(method = "lm", color = "blue") + # Add a linear regression line
  labs(
    title = "Correlation between Obesity Rate and Establishment Density by State",
    x = "Establishment Density",
    y = "Obesity Rate (%)"
  ) +
  theme_minimal()
```
### Conclusion

In conclusion, the visualizations created from the "Eating and Drinking Places by Census Tract, United States, 2003-2017" data set provided insight into the trends and dynamics of food establishments around America. Although this data set is very objective and doesn't present a direct issue, it provides the tools to devise and identify one. Through the use of time series analysis, bar plots, scatter plots, density plots, and other visualizations more information was gathered to provide a direction on identifying how establishments and other critical variables relate to socioeconomic status and obesity rates. Trends were able to be uncovered and several correlations were visualized to further explore this research area.

Overall, the visualizations have allowed us as a group to further grasp the information, making it more readable and comprehensive. This provides more insight on how the data looks and it underscores the importance of visual analysis. With the use of these visualizations, one can further analyze and understand the data as a whole. Cleaning, filtering, and visualizing the data set has laid down a foundation for further and more advanced exploration.

### Data citation (APA)

National Neighborhood Data Archive (NaNDA). (Year). Eating and Drinking Places by Census Tract, United States, 2003-2017 [Data set]. Inter-university Consortium for Political and Social Research [distributor]. https://www.openicpsr.org/openicpsr/project/115404/version/V2/view?path=/openicpsr/115404/fcr:versions/V2.2&type=project


### References

Esposito, Michael, Li, Mao, Finlay, Jessica, Gomez-Lopez, Iris, Khan, Anam, Clarke, Philippa, and Chenoweth, Megan. National Neighborhood Data Archive (NaNDA): Eating and Drinking Places by Census Tract, United States, 2003-2017. Ann Arbor, MI: Inter-university Consortium for Political and Social Research [distributor], 2020-09-16. <https://doi.org/10.3886/E115404V2>

Adult Obesity Prevalence Maps. Centers for Disease Control and Prevention. National Center for Chronic Disease Prevention and Health Promotion, Division of Nutrition, Physical Activity, and Obesity.  (21 September 2023).